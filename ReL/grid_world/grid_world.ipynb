{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment class\n",
    "class Grid:\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "        \n",
    "    def set(self, rewards, actions):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def move(self, action):\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == \"D\":\n",
    "                self.i += 1\n",
    "            elif action == \"R\":\n",
    "                self.j += 1\n",
    "            elif action == \"L\":\n",
    "                self.j -= 1\n",
    "                \n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action == \"U\":\n",
    "            self.i += 1\n",
    "        elif action == \"D\":\n",
    "            self.i -= 1\n",
    "        elif action == \"R\":\n",
    "            self.j -= 1\n",
    "        elif action == \"L\":\n",
    "            self.j += 1\n",
    "        \n",
    "        assert(self.current_state() in self.all_states())\n",
    "        \n",
    "    def game_over(self):\n",
    "        return (self.i, self.j) not in self.actions\n",
    "    \n",
    "    def all_states(self):\n",
    "        return set(self.actions.keys() | self.rewards.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid description\n",
    "def standard_grid():\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (0, 2))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('D', 'U'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'U', 'R'),\n",
    "        (2, 3): ('L', 'U')\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    \n",
    "    return g\n",
    "\n",
    "# a negative grid to limit move times\n",
    "def negative_grid(step_cost = -0.1):\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "        (0, 0): step_cost,\n",
    "        (0, 1): step_cost,\n",
    "        (0, 2): step_cost,\n",
    "        (1, 0): step_cost,\n",
    "        (1, 2): step_cost,\n",
    "        (2, 0): step_cost,\n",
    "        (2, 1): step_cost,\n",
    "        (2, 2): step_cost,\n",
    "        (2, 3): step_cost\n",
    "    })\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two print functions to display iteration results\n",
    "def print_value(V, g):\n",
    "    for i in range(g.rows):\n",
    "        print('---------------------------')\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\") # end=\"\" instead of '\\n', so the next print wouldn't start a new line\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # '-' takes an extra space\n",
    "        print(\"\")\n",
    "        \n",
    "def print_policy(P, g):\n",
    "    for i in range(g.rows):\n",
    "        print('---------------------------')\n",
    "        for j in range(g.cols):\n",
    "            p = P.get((i, j), ' ')\n",
    "            print(\"  %s  |\" % p, end=\"\")\n",
    "        print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we already know the whole model of Markov decision process (especially the transformation matrix), then we can apply dynamic programming to find the best policy. Dynamic programming typically demands much computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration for policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converge threshold\n",
    "iteration_thres = 1e-3\n",
    "\n",
    "\n",
    "def uniform_iteration():\n",
    "    g = standard_grid()\n",
    "    states = g.all_states()\n",
    "    \n",
    "    V = {}\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "    \n",
    "    gamma = 1.0\n",
    "    \n",
    "    while True:\n",
    "        max_diff = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            if s in g.actions:\n",
    "                new_v = 0\n",
    "                for a in g.actions[s]:\n",
    "                    g.set_state(s)\n",
    "                    r = g.move(a)\n",
    "                    new_v += 1.0/len(g.actions[s]) * (r + gamma * V[g.current_state()])\n",
    "                V[s] = new_v\n",
    "                max_diff = max(max_diff, np.abs(V[s] - old_v))\n",
    "        if max_diff <= iteration_thres:\n",
    "            break\n",
    "    \n",
    "    print(\"values for uniformly random actions:\")\n",
    "    print_value(V, g)\n",
    "    print(\"\\n\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_iteration(p):\n",
    "    g = standard_grid()\n",
    "    states = g.all_states()\n",
    "    \n",
    "    print(\"policy for fixed policy:\")\n",
    "    print_policy(p, g)\n",
    "    \n",
    "    V = {}\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "    \n",
    "    gamma = 0.9\n",
    "    \n",
    "    while True:\n",
    "        max_diff = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            if s in p:\n",
    "                g.set_state(s)\n",
    "                a = p[s]\n",
    "                r = g.move(a)\n",
    "                new_v = (r + gamma * V[g.current_state()])\n",
    "                V[s] = new_v\n",
    "                max_diff = max(max_diff, np.abs(V[s] - old_v))\n",
    "        if max_diff <= iteration_thres:\n",
    "            break\n",
    "        \n",
    "    print(\"values for fixed policy:\")\n",
    "    print_value(V, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values for uniformly random actions:\n",
      "---------------------------\n",
      "-0.03| 0.09| 0.22| 0.00|\n",
      "---------------------------\n",
      "-0.16| 0.00|-0.44| 0.00|\n",
      "---------------------------\n",
      "-0.29|-0.41|-0.54|-0.77|\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# uniform iteration\n",
    "uniform_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy for fixed policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n",
      "\n",
      "values for fixed policy:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n"
     ]
    }
   ],
   "source": [
    "# fixed policy iteration\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U'\n",
    "}\n",
    "\n",
    "fix_iteration(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly init a policy, then improve it by changing the action and check the value\n",
    "iteration_thres = 1e-3\n",
    "gamma = 0.9\n",
    "all_actions = ['U', 'D', 'L', 'R']\n",
    "\n",
    "def policy_iteration():\n",
    "    g = negative_grid()\n",
    "    \n",
    "    print('rewards: ')\n",
    "    print_value(g.rewards, g)\n",
    "    \n",
    "    # init policy\n",
    "    p = {}\n",
    "    for c in g.actions.keys():\n",
    "        p[c] = np.random.choice(all_actions)\n",
    "    print('init policy:')\n",
    "    print_policy(p, g)\n",
    "    \n",
    "        \n",
    "    # init value\n",
    "    V = {}\n",
    "    for s in g.all_states():\n",
    "        if s in g.actions:\n",
    "            V[s] = np.random.random()\n",
    "        else:\n",
    "            V[s] = 0\n",
    "    \n",
    "    states = g.all_states()\n",
    "    # policy iteration\n",
    "    while True:\n",
    "        # policy evaluation\n",
    "        while True:\n",
    "            max_diff = 0\n",
    "            for s in states:\n",
    "                if s in g.actions:\n",
    "                    g.set_state(s)\n",
    "                    old_v = V[s]\n",
    "                    a = p[s]\n",
    "                    r = g.move(a)\n",
    "                    V[s] = (r + gamma * V[g.current_state()])\n",
    "                    max_diff = max(max_diff, np.abs(V[s] - old_v))\n",
    "            \n",
    "            if max_diff <= iteration_thres:\n",
    "                break\n",
    "        \n",
    "        # policy improvement\n",
    "        is_policy_converge = True\n",
    "        for s in states:\n",
    "            if s in g.actions:\n",
    "                old_a = p[s]\n",
    "                new_a = None\n",
    "                best_v = float('-inf')\n",
    "                for a in all_actions:\n",
    "                    g.set_state(s)\n",
    "                    r = g.move(a)\n",
    "                    new_v = (r + gamma * V[g.current_state()])\n",
    "                    if new_v > best_v:\n",
    "                        best_v = new_v\n",
    "                        new_a = a\n",
    "                p[s] = new_a\n",
    "                if new_a != old_a:\n",
    "                    is_policy_converge = False # only become True when all the s without action change\n",
    "        \n",
    "        if is_policy_converge:\n",
    "            break\n",
    "        \n",
    "    print('values:')\n",
    "    print_value(V, g)\n",
    "    print('policy:')\n",
    "    print_policy(p, g)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: \n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "init policy:\n",
      "---------------------------\n",
      "  U  |  D  |  U  |     |\n",
      "---------------------------\n",
      "  L  |     |  D  |     |\n",
      "---------------------------\n",
      "  U  |  U  |  R  |  D  |\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration tries to loop between 'evaluate the value of a policy' and 'improve the policy with the value', while value iteration tries to find a roughly good value, then loop the policy using that value to find an optimal policy. Theoretically, value iteration is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_thres = 1e-3\n",
    "gamma = 0.9\n",
    "all_actions = ['U', 'D', 'L', 'R']\n",
    "\n",
    "def value_iteration():\n",
    "    g = negative_grid()\n",
    "    \n",
    "    print('rewards: ')\n",
    "    print_value(g.rewards, g)\n",
    "    \n",
    "    # init policy\n",
    "    p = {}\n",
    "    for c in g.actions.keys():\n",
    "        p[c] = np.random.choice(all_actions)\n",
    "    print('init policy:')\n",
    "    print_policy(p, g)\n",
    "    \n",
    "        \n",
    "    # init value\n",
    "    V = {}\n",
    "    for s in g.all_states():\n",
    "        if s in g.actions:\n",
    "            V[s] = np.random.random()\n",
    "        else:\n",
    "            V[s] = 0\n",
    "    \n",
    "    states = g.all_states()\n",
    "    # find V\n",
    "    while True:\n",
    "        max_diff = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            if s in g.actions:\n",
    "                best_v = float('-inf')\n",
    "                for a in all_actions:\n",
    "                    g.set_state(s)\n",
    "                    r = g.move(a)\n",
    "                    new_v = (r + gamma * V[g.current_state()])\n",
    "                    best_v = max(best_v, new_v)\n",
    "                V[s] = best_v\n",
    "                max_diff = max(max_diff, np.abs(V[s] - old_v))\n",
    "        if max_diff <= iteration_thres:\n",
    "            break\n",
    "    \n",
    "    # value iteration\n",
    "    for s in g.actions:\n",
    "        best_v = float('-inf')\n",
    "        best_a = None\n",
    "        for a in all_actions:\n",
    "            g.set_state(s)\n",
    "            r = g.move(a)\n",
    "            new_v = (r + gamma * V[g.current_state()])\n",
    "            if new_v > best_v:\n",
    "                best_v = new_v\n",
    "                best_a = a\n",
    "        p[s] = best_a\n",
    "            \n",
    "    print('values:')\n",
    "    print_value(V, g)\n",
    "    print('policy:')\n",
    "    print_policy(p, g)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: \n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "init policy:\n",
      "---------------------------\n",
      "  L  |  U  |  D  |     |\n",
      "---------------------------\n",
      "  D  |     |  D  |     |\n",
      "---------------------------\n",
      "  D  |  R  |  U  |  L  |\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "value_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't know the transformation matrix of the MDP, we can only let the agent try all those actions and calculate the value from its experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo method with Exploring Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring starts method works for those situations that one game can have couple of starting points. Each state-action pair has a non-zero probability to be selected as starting point, then we can greedily choose the best action to improve our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "all_actions = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def play_game_es(g, p):\n",
    "    # randomly pick one state\n",
    "    starting_state_idx = np.random.choice(len(list(g.actions.keys())))\n",
    "    g.set_state(list(g.actions.keys())[starting_state_idx])\n",
    "    \n",
    "    # randomly pick one action\n",
    "    s = g.current_state()\n",
    "    a = np.random.choice(all_actions)\n",
    "    \n",
    "    # episode, notice the states_actions_rewards stores (s(t), a(t), r(t-1)), r comes from the previous s and a\n",
    "    states_actions_rewards = [(s, a, 0)]\n",
    "    seen_states = set()\n",
    "    seen_states.add(s)\n",
    "    num_steps = 0\n",
    "    \n",
    "    while True:\n",
    "        r = g.move(a)\n",
    "        s = g.current_state()\n",
    "        num_steps += 1\n",
    "        \n",
    "        if s in seen_states:\n",
    "            reward = -10./num_steps\n",
    "            states_actions_rewards.append((s, None, reward))\n",
    "            break\n",
    "        elif g.game_over():\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "            break\n",
    "        else:\n",
    "            a = p[s]\n",
    "            states_actions_rewards.append((s, a, r))\n",
    "        seen_states.add(s)\n",
    "    \n",
    "    # calculate returns by reversing the states_actions_rewards\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    \n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        if first:\n",
    "            # we don't want to put the end state as it doesn't have any move\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + gamma * G\n",
    "    states_actions_returns.reverse()\n",
    "    \n",
    "    return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to return the max value from a dictionary\n",
    "def max_dict(d):\n",
    "    max_key = None\n",
    "    max_v = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_v:\n",
    "            max_key = k\n",
    "            max_v = v\n",
    "    return max_key, max_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo policy optimization with exploring starts\n",
    "def MC_es():\n",
    "    g = negative_grid(step_cost=-0.9)\n",
    "    \n",
    "    # init policy\n",
    "    p = {}\n",
    "    for s in g.actions:\n",
    "        p[s] = np.random.choice(all_actions)\n",
    "    \n",
    "    # init Q(s, a)\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    states = g.all_states()\n",
    "    for s in states:\n",
    "        if s in g.actions:\n",
    "            Q[s] = {}\n",
    "            for a in all_actions:\n",
    "                Q[s][a] = 0\n",
    "                returns[(s, a)] = []\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # for printing\n",
    "    deltas = []\n",
    "    for t in range(2000):\n",
    "        if t % 100 == 0:\n",
    "            print(t)\n",
    "        \n",
    "        # episode manipulation\n",
    "        max_diff = 0\n",
    "        states_actions_returns = play_game_es(g, p)\n",
    "        seen_state_action = set()\n",
    "        for s, a, G in states_actions_returns:\n",
    "            sa = (s, a)\n",
    "            if sa not in seen_state_action:\n",
    "                old_v = Q[s][a]\n",
    "                returns[sa].append(G)\n",
    "                Q[s][a] = np.mean(returns[sa])\n",
    "                max_diff = max(max_diff, np.abs(Q[s][a] - old_v))\n",
    "                seen_state_action.add(sa)\n",
    "        deltas.append(max_diff)\n",
    "        \n",
    "        # policy improve\n",
    "        for s in p.keys():\n",
    "            p[s] = max_dict(Q[s])[0]\n",
    "        \n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"final policy:\")\n",
    "    print_policy(p, g)\n",
    "    \n",
    "    V = {}\n",
    "    for s, Qs in Q.items():\n",
    "        V[s] = max_dict(Q[s])[1]\n",
    "    \n",
    "    print(\"final values:\")\n",
    "    print_value(V, g)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "\n",
      "final values:\n",
      "---------------------------\n",
      "-1.98|-1.08| 1.00| 0.00|\n",
      "---------------------------\n",
      "-3.11| 0.00|-0.77| 0.00|\n",
      "---------------------------\n",
      "-3.19|-2.66|-1.89|-1.00|\n"
     ]
    }
   ],
   "source": [
    "MC_es()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
