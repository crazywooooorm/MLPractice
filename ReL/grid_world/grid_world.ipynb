{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment class\n",
    "class Grid:\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "        \n",
    "    def set(self, rewards, actions):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def move(self, action):\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == \"D\":\n",
    "                self.i += 1\n",
    "            elif action == \"R\":\n",
    "                self.j += 1\n",
    "            elif action == \"L\":\n",
    "                self.j -= 1\n",
    "                \n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action == \"U\":\n",
    "            self.i += 1\n",
    "        elif action == \"D\":\n",
    "            self.i -= 1\n",
    "        elif action == \"R\":\n",
    "            self.j -= 1\n",
    "        elif action == \"L\":\n",
    "            self.j += 1\n",
    "        \n",
    "        assert(self.current_state() in self.all_states())\n",
    "        \n",
    "    def game_over(self):\n",
    "        return (self.i, self.j) not in self.actions\n",
    "    \n",
    "    def all_states(self):\n",
    "        return set(self.actions.keys() | self.rewards.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid description\n",
    "def standard_grid():\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (0, 2))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('D', 'U'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'U', 'R'),\n",
    "        (2, 3): ('L', 'U')\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    \n",
    "    return g\n",
    "\n",
    "# a negative grid to limit move times\n",
    "def negative_grid(step_cost = -0.1):\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "        (0, 0): step_cost,\n",
    "        (0, 1): step_cost,\n",
    "        (0, 2): step_cost,\n",
    "        (1, 0): step_cost,\n",
    "        (1, 2): step_cost,\n",
    "        (2, 0): step_cost,\n",
    "        (2, 1): step_cost,\n",
    "        (2, 2): step_cost,\n",
    "        (2, 3): step_cost\n",
    "    })\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two print functions to display iteration results\n",
    "def print_value(V, g):\n",
    "    for i in range(g.rows):\n",
    "        print('---------------------------')\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\") # end=\"\" instead of '\\n', so the next print wouldn't start a new line\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # '-' takes an extra space\n",
    "        print(\"\")\n",
    "        \n",
    "def print_policy(P, g):\n",
    "    for i in range(g.rows):\n",
    "        print('---------------------------')\n",
    "        for j in range(g.cols):\n",
    "            p = P.get((i, j), ' ')\n",
    "            print(\"  %s  |\" % p, end=\"\")\n",
    "        print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we already know the whole model of Markov decision process (especially the transformation matrix), then we can apply dynamic programming to find the best policy. Dynamic programming typically demands much computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration for policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converge threshold\n",
    "iteration_thres = 1e-3\n",
    "\n",
    "\n",
    "def uniform_iteration():\n",
    "    g = standard_grid()\n",
    "    states = g.all_states()\n",
    "    \n",
    "    V = {}\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "    \n",
    "    gamma = 1.0\n",
    "    \n",
    "    while True:\n",
    "        max_diff = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            if s in g.actions:\n",
    "                new_v = 0\n",
    "                for a in g.actions[s]:\n",
    "                    g.set_state(s)\n",
    "                    r = g.move(a)\n",
    "                    new_v += 1.0/len(g.actions[s]) * (r + gamma * V[g.current_state()])\n",
    "                V[s] = new_v\n",
    "                max_diff = max(max_diff, np.abs(V[s] - old_v))\n",
    "        if max_diff <= iteration_thres:\n",
    "            break\n",
    "    \n",
    "    print(\"values for uniformly random actions:\")\n",
    "    print_value(V, g)\n",
    "    print(\"\\n\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_iteration(p):\n",
    "    g = standard_grid()\n",
    "    states = g.all_states()\n",
    "    \n",
    "    print(\"policy for fixed policy:\")\n",
    "    print_policy(p, g)\n",
    "    \n",
    "    V = {}\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "    \n",
    "    gamma = 0.9\n",
    "    \n",
    "    while True:\n",
    "        max_diff = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            if s in p:\n",
    "                g.set_state(s)\n",
    "                a = p[s]\n",
    "                r = g.move(a)\n",
    "                new_v = (r + gamma * V[g.current_state()])\n",
    "                V[s] = new_v\n",
    "                max_diff = max(max_diff, np.abs(V[s] - old_v))\n",
    "        if max_diff <= iteration_thres:\n",
    "            break\n",
    "        \n",
    "    print(\"values for fixed policy:\")\n",
    "    print_value(V, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values for uniformly random actions:\n",
      "---------------------------\n",
      "-0.03| 0.09| 0.22| 0.00|\n",
      "---------------------------\n",
      "-0.16| 0.00|-0.44| 0.00|\n",
      "---------------------------\n",
      "-0.29|-0.41|-0.54|-0.77|\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# uniform iteration\n",
    "uniform_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy for fixed policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n",
      "\n",
      "values for fixed policy:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n"
     ]
    }
   ],
   "source": [
    "# fixed policy iteration\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U'\n",
    "}\n",
    "\n",
    "fix_iteration(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly init a policy, then improve it by changing the action and check the value\n",
    "iteration_thres = 1e-3\n",
    "gamma = 0.9\n",
    "all_actions = ['U', 'D', 'L', 'R']\n",
    "\n",
    "def policy_iteration():\n",
    "    g = negative_grid()\n",
    "    \n",
    "    print('rewards: ')\n",
    "    print_value(g.rewards, g)\n",
    "    \n",
    "    # init policy\n",
    "    p = {}\n",
    "    for c in g.actions.keys():\n",
    "        p[c] = np.random.choice(all_actions)\n",
    "    print('init policy:')\n",
    "    print_policy(p, g)\n",
    "    \n",
    "        \n",
    "    # init value\n",
    "    V = {}\n",
    "    for s in g.all_states():\n",
    "        if s in g.actions:\n",
    "            V[s] = np.random.random()\n",
    "        else:\n",
    "            V[s] = 0\n",
    "    \n",
    "    states = g.all_states()\n",
    "    # policy iteration\n",
    "    while True:\n",
    "        # policy evaluation\n",
    "        while True:\n",
    "            max_diff = 0\n",
    "            for s in states:\n",
    "                if s in g.actions:\n",
    "                    g.set_state(s)\n",
    "                    old_v = V[s]\n",
    "                    a = p[s]\n",
    "                    r = g.move(a)\n",
    "                    V[s] = (r + gamma * V[g.current_state()])\n",
    "                    max_diff = max(max_diff, np.abs(V[s] - old_v))\n",
    "            \n",
    "            if max_diff <= iteration_thres:\n",
    "                break\n",
    "        \n",
    "        # policy improvement\n",
    "        is_policy_converge = True\n",
    "        for s in states:\n",
    "            if s in g.actions:\n",
    "                old_a = p[s]\n",
    "                new_a = None\n",
    "                best_v = float('-inf')\n",
    "                for a in all_actions:\n",
    "                    g.set_state(s)\n",
    "                    r = g.move(a)\n",
    "                    new_v = (r + gamma * V[g.current_state()])\n",
    "                    if new_v > best_v:\n",
    "                        best_v = new_v\n",
    "                        new_a = a\n",
    "                p[s] = new_a\n",
    "                if new_a != old_a:\n",
    "                    is_policy_converge = False # only become True when all the s without action change\n",
    "        \n",
    "        if is_policy_converge:\n",
    "            break\n",
    "        \n",
    "    print('values:')\n",
    "    print_value(V, g)\n",
    "    print('policy:')\n",
    "    print_policy(p, g)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: \n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "init policy:\n",
      "---------------------------\n",
      "  U  |  D  |  U  |     |\n",
      "---------------------------\n",
      "  L  |     |  D  |     |\n",
      "---------------------------\n",
      "  U  |  U  |  R  |  D  |\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration tries to loop between 'evaluate the value of a policy' and 'improve the policy with the value', while value iteration tries to find a roughly good value, then loop the policy using that value to find an optimal policy. Theoretically, value iteration is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_thres = 1e-3\n",
    "gamma = 0.9\n",
    "all_actions = ['U', 'D', 'L', 'R']\n",
    "\n",
    "def value_iteration():\n",
    "    g = negative_grid()\n",
    "    \n",
    "    print('rewards: ')\n",
    "    print_value(g.rewards, g)\n",
    "    \n",
    "    # init policy\n",
    "    p = {}\n",
    "    for c in g.actions.keys():\n",
    "        p[c] = np.random.choice(all_actions)\n",
    "    print('init policy:')\n",
    "    print_policy(p, g)\n",
    "    \n",
    "        \n",
    "    # init value\n",
    "    V = {}\n",
    "    for s in g.all_states():\n",
    "        if s in g.actions:\n",
    "            V[s] = np.random.random()\n",
    "        else:\n",
    "            V[s] = 0\n",
    "    \n",
    "    states = g.all_states()\n",
    "    # find V\n",
    "    while True:\n",
    "        max_diff = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            if s in g.actions:\n",
    "                best_v = float('-inf')\n",
    "                for a in all_actions:\n",
    "                    g.set_state(s)\n",
    "                    r = g.move(a)\n",
    "                    new_v = (r + gamma * V[g.current_state()])\n",
    "                    best_v = max(best_v, new_v)\n",
    "                V[s] = best_v\n",
    "                max_diff = max(max_diff, np.abs(V[s] - old_v))\n",
    "        if max_diff <= iteration_thres:\n",
    "            break\n",
    "    \n",
    "    # value iteration\n",
    "    for s in g.actions:\n",
    "        best_v = float('-inf')\n",
    "        best_a = None\n",
    "        for a in all_actions:\n",
    "            g.set_state(s)\n",
    "            r = g.move(a)\n",
    "            new_v = (r + gamma * V[g.current_state()])\n",
    "            if new_v > best_v:\n",
    "                best_v = new_v\n",
    "                best_a = a\n",
    "        p[s] = best_a\n",
    "            \n",
    "    print('values:')\n",
    "    print_value(V, g)\n",
    "    print('policy:')\n",
    "    print_policy(p, g)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: \n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "init policy:\n",
      "---------------------------\n",
      "  L  |  U  |  D  |     |\n",
      "---------------------------\n",
      "  D  |     |  D  |     |\n",
      "---------------------------\n",
      "  D  |  R  |  U  |  L  |\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "value_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't know the transformation matrix of the MDP, we can only let the agent try all those actions and calculate the value from its experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo method with Exploring Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring starts method works for those situations that one game can have couple of starting points. Each state-action pair has a non-zero probability to be selected as starting point, then we can greedily choose the best action to improve our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "all_actions = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def play_game_es(g, p):\n",
    "    # randomly pick one state\n",
    "    starting_state_idx = np.random.choice(len(list(g.actions.keys())))\n",
    "    g.set_state(list(g.actions.keys())[starting_state_idx])\n",
    "    \n",
    "    # randomly pick one action\n",
    "    s = g.current_state()\n",
    "    a = np.random.choice(all_actions)\n",
    "    \n",
    "    # episode, notice the states_actions_rewards stores (s(t), a(t), r(t-1)), r comes from the previous s and a\n",
    "    states_actions_rewards = [(s, a, 0)]\n",
    "    seen_states = set()\n",
    "    seen_states.add(s)\n",
    "    num_steps = 0\n",
    "    \n",
    "    while True:\n",
    "        r = g.move(a)\n",
    "        s = g.current_state()\n",
    "        num_steps += 1\n",
    "        \n",
    "        if s in seen_states:\n",
    "            reward = -10./num_steps\n",
    "            states_actions_rewards.append((s, None, reward))\n",
    "            break\n",
    "        elif g.game_over():\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "            break\n",
    "        else:\n",
    "            a = p[s]\n",
    "            states_actions_rewards.append((s, a, r))\n",
    "        seen_states.add(s)\n",
    "    \n",
    "    # calculate returns by reversing the states_actions_rewards\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    \n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        if first:\n",
    "            # we don't want to put the end state as it doesn't have any move\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + gamma * G\n",
    "    states_actions_returns.reverse()\n",
    "    \n",
    "    return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to return the max value from a dictionary\n",
    "def max_dict(d):\n",
    "    max_key = None\n",
    "    max_v = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_v:\n",
    "            max_key = k\n",
    "            max_v = v\n",
    "    return max_key, max_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo policy optimization with exploring starts\n",
    "def MC_es():\n",
    "    g = negative_grid(step_cost=-0.9)\n",
    "    \n",
    "    # init policy\n",
    "    p = {}\n",
    "    for s in g.actions:\n",
    "        p[s] = np.random.choice(all_actions)\n",
    "    \n",
    "    # init Q(s, a)\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    states = g.all_states()\n",
    "    for s in states:\n",
    "        if s in g.actions:\n",
    "            Q[s] = {}\n",
    "            for a in all_actions:\n",
    "                Q[s][a] = 0\n",
    "                returns[(s, a)] = []\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # for printing\n",
    "    deltas = []\n",
    "    for t in range(2000):\n",
    "        if t % 100 == 0:\n",
    "            print(t)\n",
    "        \n",
    "        # episode manipulation\n",
    "        max_diff = 0\n",
    "        states_actions_returns = play_game_es(g, p)\n",
    "        seen_state_action = set()\n",
    "        for s, a, G in states_actions_returns:\n",
    "            sa = (s, a)\n",
    "            if sa not in seen_state_action:\n",
    "                old_v = Q[s][a]\n",
    "                returns[sa].append(G)\n",
    "                Q[s][a] = np.mean(returns[sa])\n",
    "                max_diff = max(max_diff, np.abs(Q[s][a] - old_v))\n",
    "                seen_state_action.add(sa)\n",
    "        deltas.append(max_diff)\n",
    "        \n",
    "        # policy improve\n",
    "        for s in p.keys():\n",
    "            p[s] = max_dict(Q[s])[0]\n",
    "        \n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"final policy:\")\n",
    "    print_policy(p, g)\n",
    "    \n",
    "    V = {}\n",
    "    for s, Qs in Q.items():\n",
    "        V[s] = max_dict(Q[s])[1]\n",
    "    \n",
    "    print(\"final values:\")\n",
    "    print_value(V, g)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "\n",
      "final values:\n",
      "---------------------------\n",
      "-1.98|-1.08| 1.00| 0.00|\n",
      "---------------------------\n",
      "-3.11| 0.00|-0.77| 0.00|\n",
      "---------------------------\n",
      "-3.19|-2.66|-1.89|-1.00|\n"
     ]
    }
   ],
   "source": [
    "MC_es()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo on-policy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases we can only start the game with a fixed starting point, which makes exploring starts not feasible, we use on-policy first-visit MC instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "all_actions = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# using epsilon method to pick action\n",
    "def random_action(a, eps = 0.1):\n",
    "    p = np.random.random()\n",
    "    if p < eps:\n",
    "        return np.random.choice(all_actions)\n",
    "    else:\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_no_es(g, p):\n",
    "    # fix starting point\n",
    "    s = (2, 0)\n",
    "    g.set_state(s)\n",
    "    a = random_action(p[s]) \n",
    "    states_actions_rewards = [(s, a, 0)]\n",
    "    seen_states = set()\n",
    "    seen_states.add(s)\n",
    "    \n",
    "    while True:\n",
    "        r = g.move(a)\n",
    "        s = g.current_state()\n",
    "        # given the epsilon setting, the agent shouldn't end up bumping the wall all time\n",
    "        # but implementing this can help increase the efficiency\n",
    "        # give a very big punishment for bumping to wall\n",
    "        if s in seen_states:\n",
    "            states_actions_rewards.append((s, None, -10))\n",
    "            break\n",
    "        elif g.game_over():\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "            break\n",
    "        else:\n",
    "            a = random_action(p[s])\n",
    "            states_actions_rewards.append((s, a, r))\n",
    "            seen_states.add(s)\n",
    "            \n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + gamma * G\n",
    "    \n",
    "    states_actions_returns.reverse()\n",
    "    return states_actions_returns     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_no_es():\n",
    "    g = negative_grid(step_cost=0.9)\n",
    "    p = {}\n",
    "    for s in g.actions:\n",
    "        p[s] = np.random.choice(all_actions)\n",
    "        \n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    for s in g.all_states():\n",
    "        if s in g.actions:\n",
    "            Q[s] = {}\n",
    "            for a in all_actions:\n",
    "                Q[s][a] = 0\n",
    "                returns[(s, a)] = []\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    deltas = []\n",
    "    for t in range(4000):\n",
    "        if t % 100 == 0:\n",
    "            print(t)\n",
    "        \n",
    "        max_diff = 0\n",
    "        states_actions_returns = play_game_no_es(g, p)\n",
    "        seen_state_action = set()\n",
    "        \n",
    "        for s, a, G in states_actions_returns:\n",
    "            if (s, a) not in seen_state_action:\n",
    "                old_q = Q[s][a]\n",
    "                returns[(s, a)].append(G)\n",
    "                Q[s][a] = np.mean(returns[(s, a)])\n",
    "                max_diff = max(max_diff, np.abs(Q[s][a] - old_q))\n",
    "                seen_state_action.add((s, a))\n",
    "        deltas.append(max_diff)\n",
    "        \n",
    "        for s in p:\n",
    "            p[s] = max_dict(Q[s])[0]\n",
    "            \n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "    \n",
    "    V = {}\n",
    "    for s in p:\n",
    "        V[s] = max_dict(Q[s])[1]\n",
    "    \n",
    "    print(\"final values:\")\n",
    "    print_value(V, g)\n",
    "    \n",
    "    print(\"final policy:\")\n",
    "    print_policy(p, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final values:\n",
      "---------------------------\n",
      " 1.26| 1.24| 1.00| 0.00|\n",
      "---------------------------\n",
      " 1.31| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 1.29|-0.96|-1.03|-1.00|\n",
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MC_no_es()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal difference learning is also a model-free algorithm, the difference from Monte Carlo method is it starts value evaluation during the episode rather than after episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA (state–action–reward–state–action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "all_actions = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def td_sarsa():\n",
    "    g = negative_grid(step_cost = -0.1)\n",
    "    \n",
    "    Q = {}\n",
    "    for s in g.all_states():\n",
    "        Q[s] = {}\n",
    "        for a in all_actions:\n",
    "            Q[s][a] = 0\n",
    "    \n",
    "    # track Q updated times\n",
    "    update_counts = {}\n",
    "    update_counts_sa = {}\n",
    "    for s in g.all_states():\n",
    "        update_counts_sa[s] = {}\n",
    "        for a in all_actions:\n",
    "            update_counts_sa[s][a] = 1.0\n",
    "            \n",
    "    # repeat until convergence\n",
    "    t = 1.0\n",
    "    deltas = []\n",
    "    for it in range(10000):\n",
    "        if it % 100 == 0:\n",
    "            t += 1e-2\n",
    "        if it % 2000 == 0:\n",
    "            print(\"it:\", it)\n",
    "        \n",
    "        # starting point\n",
    "        s = (2, 0)\n",
    "        g.set_state(s)\n",
    "        \n",
    "        a = max_dict(Q[s])[0]\n",
    "        a = random_action(a, eps=0.5/t)\n",
    "        \n",
    "        max_diff = 0\n",
    "        while not g.game_over():\n",
    "            r = g.move(a)\n",
    "            s2 = g.current_state()\n",
    "            \n",
    "            # updating Q(s,a) needs Q(s',a')\n",
    "            a2 = max_dict(Q[s2])[0]\n",
    "            a2 = random_action(a2, eps=0.5/t)\n",
    "            \n",
    "            alpha_use = alpha / update_counts_sa[s][a]\n",
    "            update_counts_sa[s][a] += 0.005\n",
    "            old_qsa = Q[s][a]\n",
    "            Q[s][a] = Q[s][a] + alpha_use*(r + gamma*Q[s2][a2] - Q[s][a])\n",
    "            max_diff = max(max_diff, np.abs(Q[s][a] - old_qsa))\n",
    "            \n",
    "            update_counts[s] = update_counts.get(s, 0) + 1\n",
    "            \n",
    "            s = s2\n",
    "            a = a2\n",
    "        \n",
    "        deltas.append(max_diff)\n",
    "        \n",
    "    # policy optimize\n",
    "    p = {}\n",
    "    V = {}\n",
    "    for s in g.actions:\n",
    "        a, max_q = max_dict(Q[s])\n",
    "        p[s] = a\n",
    "        V[s] = max_q\n",
    "            \n",
    "    # proportion of time we spend updating each part of Q\n",
    "    print(\"update counts:\")\n",
    "    total = np.sum(list(update_counts.values()))\n",
    "    for k, v in update_counts.items():\n",
    "        update_counts[k] = float(v) / total\n",
    "    print_value(update_counts, g)\n",
    "    \n",
    "    print('Value')\n",
    "    print_value(V, g)\n",
    "    print('Policy')\n",
    "    print_policy(p, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0\n",
      "it: 2000\n",
      "it: 4000\n",
      "it: 6000\n",
      "it: 8000\n",
      "update counts:\n",
      "---------------------------\n",
      " 0.18| 0.19| 0.18| 0.00|\n",
      "---------------------------\n",
      " 0.18| 0.00| 0.04| 0.00|\n",
      "---------------------------\n",
      " 0.19| 0.02| 0.02| 0.00|\n",
      "Value\n",
      "---------------------------\n",
      " 0.50| 0.71| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.30| 0.00| 0.71| 0.00|\n",
      "---------------------------\n",
      " 0.13| 0.17| 0.37| 0.17|\n",
      "Policy\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "td_sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
